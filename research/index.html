<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Chris Xiaoxuan Lu


  | research

</title>
<meta name="description" content="Associate Professor at UCL
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://combinatronics.io/jwarby/pygments-css/master/.css" media="none" id="highlight_theme_light" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://christopherlu.github.io/research/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://ChristopherLu.github.io/">
       <span class="font-weight-bold">Chris</span> Xiaoxuan  Lu
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li> -->
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/research/">
                research
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/team/">
                team
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/vacancies/">
                vacancies
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                submenus
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/teaching/">teaching</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/vacancies/">vacancies</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/links/">links</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/contact/">contact</a>
              
              
              </div>
          </li>
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">research</h1>
    <p class="post-description">Ongoing and completed research projects by MAPS Lab</p>
  </header>

  <article>
    <hr>
<h2 id="scaling-foundation-models-for-enhanced-perception-in-embodied-agents"><strong>Scaling Foundation Models for Enhanced Perception in Embodied Agents</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/scale_foundation_models-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/scale_foundation_models-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/scale_foundation_models-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/scale_foundation_models.png" title="">

  </picture>

  

</figure>

    </div>
</div>

<p>Scaling large foundation models (e.g., CLIP, DINO and SAM) to local compute holds the promise of revolutionizing how embodied agents, such as robots, AR, and IoT devices, interact with and comprehend their environments. At the heart of this endeavor is the integration of advanced <strong>visual and visual-language</strong> understanding capabilities into these agents, enabling them to interpret and respond to the complex, dynamic world around them. Recent advancements in AI and machine learning have paved the way for such models to be adapted for use in real-world settings, where they can offer unparalleled context-awareness in a zero-shot way. However, deploying these foundation models in diverse and often unpredictable environments poses significant challenges. These include ensuring <strong>generalization across different sensors</strong> carried by the agents, optimizing for <strong>computational efficiency</strong> on limited hardware, and achieving <strong>seamless interaction</strong> with human users and other machines. In our research, we explore the full spectrum of these challenges, from adapting the models for efficient real-time processing to enhancing their understanding across different sensor inputs. <em>This is a new research line in the lab</em>, more research outputs are expected.</p>

<p>Research Output: [<a href="https://arxiv.org/abs/2403.04908" target="_blank" rel="noopener noreferrer">ECCVâ€™24a</a>],[<a href="https://tsagkas.github.io/click2grasp/" target="_blank" rel="noopener noreferrer">IROSâ€™24</a>], [<a href="https://tsagkas.github.io/vl-fields/" target="_blank" rel="noopener noreferrer">ICRA-Wâ€™2023</a>]</p>

<h2 id="robust-spatial-perception-for-autonomous-vehicles-in-the-wild"><strong>Robust Spatial Perception for Autonomous Vehicles in the Wild</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/av-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/av-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/av-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/av.png" title="">

  </picture>

  

</figure>

    </div>
</div>

<p>Autonomous driving, aka. mobile autonomy, promises to radically change the cityscape and save many human lives. A pillar component for achieving mobile autonomy is <strong>spatial perception - the ability for vehicles to understand the ambient environment and localize themselves on the road</strong>. Thanks to the recent advances in computer vision and solid-state technology, great improvement in spatial perception has been witnessed in controlled environments. Nevertheless, the perception robustness of many systems is still far off the safety requirement of autonomous driving when it comes to the wild condition, such as bad weather, poor illumination, various dynamic objects or even malicious attacks. In the MAPS lab, we study the robust spatial perception problems in full-stack, ranging from the single-modality based methods to multi-sensory fusion, and to the perception uncertainty quantification. More recently, a traction to my lab is unfolding the potential of <strong>4D automotive radar in localization and scene understanding</strong>. As an emerging sensor, automotive radars are reputable for their sensing robustness against bad weather and adverse illumination. However, due to their significantly lower data quality, it remains largely unknown how one can transform the radar sensing robustness to vehicle perception effectiveness - a question my team keen to answer.</p>

<p>Research Output: [<a href="https://arxiv.org/abs/2309.09737" target="_blank" rel="noopener noreferrer">ICRAâ€™2024a</a>], [<a href="https://arxiv.org/abs/2309.17336" target="_blank" rel="noopener noreferrer">ICRAâ€™2024b</a>], [<a href="https://toytiny.github.io/publication/23-cmflow-cvpr/index.html" target="_blank" rel="noopener noreferrer">CVPRâ€™2023</a>], [<a href="https://arxiv.org/abs/2112.02469" target="_blank" rel="noopener noreferrer">IROSâ€™2023</a>], [<a href="http://arxiv.org/abs/2209.14602" target="_blank" rel="noopener noreferrer">RA-Lâ€™2023</a>], [<a href="https://www.research.ed.ac.uk/en/publications/metawave-attacking-mmwave-sensing-with-meta-material-enhanced-tag" target="_blank" rel="noopener noreferrer">NDSSâ€™2023</a>], [<a href="https://arxiv.org/abs/2203.01137" target="_blank" rel="noopener noreferrer">RA-L/IROSâ€™2022</a>], [<a href="https://arxiv.org/abs/2203.01851" target="_blank" rel="noopener noreferrer">IROSâ€™2022a</a>], [<a href="https://www.pure.ed.ac.uk/ws/portalfiles/portal/289946438/Pedestrian_Liveness_LI_DOA18072022_AFV.pdf" target="_blank" rel="noopener noreferrer">SECONâ€™2022</a>], [<a href="https://intranet.csc.liv.ac.uk/~ramdrop/autoplace.html" target="_blank" rel="noopener noreferrer">ICRAâ€™2022a</a>], [<a href="https://qqqgpe.github.io/2022-02-11/DC-Loc" target="_blank" rel="noopener noreferrer">ICRAâ€™2022b</a>], [<a href="https://arxiv.org/abs/1912.13077" target="_blank" rel="noopener noreferrer">TNNLSâ€™2022</a>], [<a href="https://arxiv.org/pdf/1908.03918.pdf" target="_blank" rel="noopener noreferrer">TNNLSâ€™2021</a>], [<a href="http://arxiv.org/abs/1909.03557" target="_blank" rel="noopener noreferrer">AAAIâ€™2020</a>], [<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Selective_Sensor_Fusion_for_Neural_Visual-Inertial_Odometry_CVPR_2019_paper.html" target="_blank" rel="noopener noreferrer">CVPRâ€™2019</a>]</p>

<hr>
<h2 id="privacy-aware-and-low-cost-human-motion-sensing"><strong>Privacy-aware and Low-Cost Human Motion Sensing</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/ips-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/ips-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/ips-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/ips.png" title="">

  </picture>

  

</figure>

    </div>
</div>

<p>Despite a plethora methods being proposed over recent decades, todayâ€™s human sensing systems â€” primarily those designed to track location and monitor activities â€” often rely on RGB cameras. These methods encounter difficulties in adverse lighting conditions and raise increasing privacy concerns in domestic environments. Moving away from camera-centric solutions, the challenge intensifies when designing for <strong>low-resolution sensors, such as mmWave, WiFi, UWB, BLE, and inertial measurement units</strong>. This complexity arises because humans do not naturally interact with these modalities, complicating the application of our everyday experiences to improve algorithms for these sensors. Leveraging the latest advancements in machine learning, we advocate for <strong>AI-empowered methods</strong> to analyze data from these sensors, extending their utility. Our research also delves into whether these low-cost, low-resolution sensors can be utilized for fine-grained pose estimation, such as gestures and hand movements, beyond mere localization. Importantly, we aim to develop methods that are more acceptable by users, particularly for the aging population, ensuring our solutions are both effective and sensitive to the privacy and usability needs of this demographic.</p>

<p>Research Output: [<a href="http://arxiv.org/abs/2306.17010" target="_blank" rel="noopener noreferrer">ECCVâ€™24b</a>], [<a href="https://arxiv.org/abs/2311.10601" target="_blank" rel="noopener noreferrer">ICRAâ€™2024c</a>], [<a href="https://arxiv.org/abs/2305.10345" target="_blank" rel="noopener noreferrer">NeurIPSâ€™2023
</a>], [<a href="http://arxiv.org/abs/2207.07859" target="_blank" rel="noopener noreferrer">Patternsâ€™2023</a>], [<a href="https://www.pure.ed.ac.uk/ws/portalfiles/portal/327349383/Human_Parsing_WANG_DOA20012023_AFV_CC_BY.pdf" target="_blank" rel="noopener noreferrer">UbiCompâ€™2023</a>], [<a href="http://arxiv.org/abs/2111.03976" target="_blank" rel="noopener noreferrer">IoT-Jâ€™2023</a>], [<a href="https://arxiv.org/pdf/2207.07896.pdf" target="_blank" rel="noopener noreferrer">UbiCompâ€™2022</a>], [<a href="https://arxiv.org/pdf/2208.14326.pdf" target="_blank" rel="noopener noreferrer">IoT-Jâ€™2022</a>],[<a href="https://arxiv.org/abs/2103.01055" target="_blank" rel="noopener noreferrer">ICCVâ€™2021</a>], [<a href="https://ieeexplore.ieee.org/document/8937008" target="_blank" rel="noopener noreferrer">TMCâ€™2021</a>], [<a href="https://ieeexplore.ieee.org/document/9547669" target="_blank" rel="noopener noreferrer">TNNLSâ€™2021</a>], [<a href="https://doi.org/10.1109/ICRA40945.2020.9197437" target="_blank" rel="noopener noreferrer">ICRAâ€™2020</a>], [<a href="https://ieeexplore.ieee.org/document/8402111" target="_blank" rel="noopener noreferrer">TMCâ€™2019</a>], [<a href="https://doi.org/10.1609/aaai.v33i01.33018009" target="_blank" rel="noopener noreferrer">AAAIâ€™2019</a>], [<a href="https://doi.org/10.1109/DCOSS.2019.00028" target="_blank" rel="noopener noreferrer">DCOSSâ€™2019</a>], [<a href="http://arxiv.org/abs/1802.02209" target="_blank" rel="noopener noreferrer">AAAIâ€™2018</a>], [<a href="http://www.cs.ox.ac.uk/files/10769/%5BMobiCom2018%5Demr_slam.pdf" target="_blank" rel="noopener noreferrer">MobiComâ€™2018</a>], [<a href="https://doi.org/10.1109/TWC.2015.2487963" target="_blank" rel="noopener noreferrer">TWCâ€™2016</a>]</p>

<hr>
<h2 id="robust-and-rapid-sense-augmentation-support-for-first-responders-completed"><strong>Robust and Rapid Sense Augmentation Support for First Responders (completed)</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/firefighters-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/firefighters-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/firefighters-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/firefighters.png" title="">

  </picture>

  

</figure>

    </div>
</div>
<!-- <div class="caption">
    Example Submodular Settings
</div> -->

<p>From the equator to the Arctic, fire disasters are going to happen more often as a result of anthropogenic climate change. This consequently results in more frequent duty calls of firefighters. However, at the present, firefighting is still regarded as one of the most strenuous and dangerous jobs in the world. A fire incident is often accompanied by a variety of airborne obscurants (e.g., smoke and dust) and poor illumination, making firefighters difficult to navigate themselves and understand the fire ground. We aim to design robust yet real-time <strong>localization, mapping and scene understanding</strong> services that can be directly integrated into the <strong>augmented reality wearables and firefighting robots</strong>. These support systems will, in turn, enhance firefightersâ€™ operational capacity and safety in visually-degraded conditions and on resource-constrained platforms.</p>

<p>Research Output: [<a href="https://arxiv.org/abs/2307.03623" target="_blank" rel="noopener noreferrer">EWSNâ€™2023</a>], [<a href="https://arxiv.org/abs/2104.07196" target="_blank" rel="noopener noreferrer">TR-Oâ€™2022</a>], [<a href="https://arxiv.org/abs/2206.01589" target="_blank" rel="noopener noreferrer">IROSâ€™2022b</a>], [<a href="https://arxiv.org/pdf/2112.05665.pdf" target="_blank" rel="noopener noreferrer">CPS-ERâ€™2022</a>], [<a href="https://ieeexplore.ieee.org/document/9561738" target="_blank" rel="noopener noreferrer">ICRAâ€™2021</a>], [<a href="https://arxiv.org/abs/2006.02266" target="_blank" rel="noopener noreferrer">SenSysâ€™2020</a>], [<a href="https://arxiv.org/abs/1911.00398" target="_blank" rel="noopener noreferrer">MobiSysâ€™2020</a>], [<a href="https://ieeexplore.ieee.org/document/8968430" target="_blank" rel="noopener noreferrer">RA-Lâ€™2020</a>]</p>

<p>Media Exposure: <a href="https://www.bbc.co.uk/news/av/uk-scotland-63075749" target="_blank" rel="noopener noreferrer">BBC News</a>, <a href="https://www.bbc.co.uk/sounds/play/m001cg5x" target="_blank" rel="noopener noreferrer">BBC Good
 Morning Scotland</a>, <a href="http://www.pressdata.co.uk/viewbroadcast.asp?a_id=27804277" target="_blank" rel="noopener noreferrer">STV</a>, <a href="https://planetradio.co.uk/borders/local/news/firefighters-smart-helmets-heriot-watt/" target="_blank" rel="noopener noreferrer">Planet Radio</a>, <a href="http://www.pressdata.co.uk/viewbroadcast.asp?a_id=27806734" target="_blank" rel="noopener noreferrer">Sky News</a>, <a href="https://podcasts.apple.com/gb/podcast/ai-smart-helmets-give-firefighters-superhero-ability/id1516299890?i=1000580906865" target="_blank" rel="noopener noreferrer">Evening
 Standard Tech &amp; Science Daily</a>, <a href="https://www.scottishdailyexpress.co.uk/news/scottish-news/firefighters-could-soon-smart-helmets-28099105" target="_blank" rel="noopener noreferrer">Scottish
 Daily Express</a>, <a href="https://www.independent.co.uk/news/uk/experts-scotland-edinburgh-innovation-university-of-edinburgh-b2176943.html" target="_blank" rel="noopener noreferrer">The Independent</a>, <a href="https://www.scottishfield.co.uk/living/firefighters-get-hi-tech-help-from-robotarium/" target="_blank" rel="noopener noreferrer">Scottish
 Field</a>, <a href="https://www.scottishdailyexpress.co.uk/news/scottish-news/firefighters-could-soon-smart-helmets-28099105" target="_blank" rel="noopener noreferrer">Scottish Daily Mail</a>, <a href="https://news.italy-24.com/trends/115673/Helmets-with-artificial-intelligence-to-help-firefighters.html" target="_blank" rel="noopener noreferrer">Italy
 24 News</a>, <a href="https://www.irishnews.com/magazine/technology/2022/09/28/news/firefighters_could_soon_have_smart_helmets_to_help_locate_blaze_victims-2842709/" target="_blank" rel="noopener noreferrer">Irish
 News</a>, <a href="https://eandt.theiet.org/content/articles/2022/09/smart-helmets-could-help-firefighters-locate-blaze-victims/" target="_blank" rel="noopener noreferrer">Engineering
 &amp; Technology</a>, <a href="https://www.digit.fyi/scots-firefighters-national-robotarium-smart-helmet/" target="_blank" rel="noopener noreferrer">Digit
 News</a></p>

<hr>
<h2 id="robust-identity-inference-across-digital-and-physical-worlds-completed"><strong>Robust Identity Inference across Digital and Physical Worlds (completed)</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/human_id-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/human_id-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/human_id-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/human_id.png" title="">

  </picture>

  

</figure>

    </div>
</div>

<p>Key to realizing the vision of human-centred computing is the ability for machines to recognize people, so that spaces and devices can become truly personalized. However, the unpredictability of real-world environments impacts robust recognition, limiting usability. In real conditions, human identification systems have to handle issues such as out-of-set subjects and domain deviations, where conventional supervised learning approaches for training and inference are poorly suited. With the rapid development of Internet of Things (IoT), we advocate a new labelling method that exploits signals of opportunity hidden in heterogeneous IoT data. The key insight is that <strong>one sensor modality can leverage the signals measured by other co-located sensor modalities to improve its own labelling performance</strong>. If identity associations between heterogeneous sensor data can be discovered, it is possible to automatically label data, leading to more robust human recognition, without manual labelling or enrolment. On the other side of the coin, we also study the privacy implication for such cross-modal identity association.</p>

<p>Research Output: [<a href="https://arxiv.org/abs/2001.08211" target="_blank" rel="noopener noreferrer">WWWâ€™2020</a>], [<a href="https://arxiv.org/abs/1908.09002" target="_blank" rel="noopener noreferrer">WWWâ€™2019</a>], [<a href="https://ieeexplore.ieee.org/document/8755294" target="_blank" rel="noopener noreferrer">IoT-Jâ€™2019</a>], [<a href="https://arxiv.org/abs/1912.04836" target="_blank" rel="noopener noreferrer">UbiCompâ€™2018</a>], [<a href="https://dl.acm.org/doi/10.1145/3267242.3267252" target="_blank" rel="noopener noreferrer">ISWCâ€™2018</a>], [<a href="https://dl.acm.org/doi/10.1145/3055031.3055073" target="_blank" rel="noopener noreferrer">IPSNâ€™2017</a>]</p>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2024 Chris Xiaoxuan Lu.
    
    
    
    Last updated: December 08, 2024.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
